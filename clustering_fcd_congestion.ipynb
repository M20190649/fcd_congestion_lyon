{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some basic import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import logging\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's configure our Spark environment.\n",
    "For simplicity, we will work in Spark in local settings.\n",
    "\n",
    "You just need to run the following lines of code in a pyspark ready environment (local[*] stands for \"use all the cores of your CPU\").\n",
    "\n",
    "That means you won't be able to use multiple machines, but you will be able to use the multi-processor capabilities of your CPU.\n",
    "\n",
    "If you have the possibility to use multiple machines connected to the same local network, you may want to configure a Spark standalone (distributed) cluster with one master node and a certain number of worker nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.18:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Clustering Lyon Traffic Data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Clustering Lyon Traffic Data>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remember to stop the context if you re-execute this cell multiple times:\n",
    "#sc.stop()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('Clustering Lyon Traffic Data') \\\n",
    "    .config(\"spark.network.timeout\", \"10000000\") \\\n",
    "    .config(\"spark.driver.memory\", \"30g\") \\\n",
    "    .getOrCreate() \n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's load the Lyon trips data in parquet format\n",
    "\n",
    "We are going to work on a Parquet file, containing the GPS observations for the city of Lyon, France on several months in 2017-2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the parquet files of Lyon.\n",
    "Adapt to your configuration the filepaths to the folders containing the parquet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER_LYON_TRIPS = '/Volumes/LaCie/datasets/fcd_mediamobile_2017_2018/Parquet' #adapt the path to your case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read the trips data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to measure the time it takes to perform the following operations in Spark, by using the time_usage function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "@contextmanager\n",
    "def time_usage(name=\"\"):\n",
    "    \"\"\"log the time usage in a code block\n",
    "    name: the prefix text to show\"\"\"\n",
    "    start = time.time()\n",
    "    yield\n",
    "    end = time.time()\n",
    "    elapsed_seconds = float(\"%.4f\" % (end - start))\n",
    "    logging.info('%s: elapsed seconds: %s', name, elapsed_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Reading parquet: elapsed seconds: 25.9784\n"
     ]
    }
   ],
   "source": [
    "#read the content of the parquet:\n",
    "with time_usage(\"Reading parquet\"):\n",
    "    dfTrips = spark.read.parquet(\"file:///\" + DATA_FOLDER_LYON_TRIPS)\n",
    "dfTrips = dfTrips.withColumn(\"hour\", hour(col(\"parsedTimestamp\")))\n",
    "dfTrips = dfTrips.withColumn(\"minute\", minute(col(\"parsedTimestamp\")))\n",
    "\n",
    "#show the content of the first 10 lines of the dfTrips\n",
    "#with time_usage(\"Showing the first 10 lines\"):\n",
    "#    dfTrips.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>printSchema()</code> method provides information on the dataframe, including the type associated to each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Printing schema: elapsed seconds: 0.0057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vehicleId: string (nullable = true)\n",
      " |-- trackId: long (nullable = true)\n",
      " |-- obsId: long (nullable = true)\n",
      " |-- linkId: long (nullable = true)\n",
      " |-- coverage: float (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- speedInKph: integer (nullable = true)\n",
      " |-- parsedTimestamp: timestamp (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with time_usage(\"Printing schema\"):\n",
    "    dfTrips.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Filtering trips: elapsed seconds: 0.2602\n"
     ]
    }
   ],
   "source": [
    "#temporary filtering for testing\n",
    "with time_usage(\"Filtering trips\"):\n",
    "    #Apply temporal filtering to reduce the size of your dataset. \n",
    "    \n",
    "    #dfTrips = dfTrips.filter((col(\"year\") == 2017) & (col(\"month\") == 10) & (col(\"day\") >= 15))\n",
    "    dfTrips = dfTrips.filter((col(\"year\") == 2017) | (col(\"year\") == 2018))\n",
    "    '''dfTrips = dfTrips.filter((col(\"year\") == 2019) | (col(\"year\") == 2018)\\\n",
    "                             | ((col(\"year\") == 2017) & (col(\"month\") >= 11))\\\n",
    "                             | ((col(\"year\") == 2017) & (col(\"month\") == 10) & (col(\"day\") >= 15)))'''\n",
    "    \n",
    "    #dfTrips = dfTrips.filter((col(\"year\") == 2019) | ((col(\"year\") == 2018) & (col(\"month\") >= 5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfTrips.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read the links_with_iris file now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going now to use additional information related to the road links to which our FCD observations are related, such as the commune the link belongs to, as retrieved from the file links_with_iris.csv.\n",
    "\n",
    "The file is available at this url:\n",
    "\n",
    "https://www.dropbox.com/s/mx0kom8wa754zn8/links_with_iris.csv?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINKS_FILEPATH = '/Volumes/LaCie/datasets/fcd_mediamobile_2017_2018/Arc/graph_with_iris/links_with_iris.csv'\n",
    "dfLinks = spark.read.load(LINKS_FILEPATH, format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "#dfLinks.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- iris_count: integer (nullable = true)\n",
      " |-- iris_length: double (nullable = true)\n",
      " |-- ffs: integer (nullable = true)\n",
      " |-- speedlimit: integer (nullable = true)\n",
      " |-- frc: integer (nullable = true)\n",
      " |-- netclass: integer (nullable = true)\n",
      " |-- fow: integer (nullable = true)\n",
      " |-- routenumber: string (nullable = true)\n",
      " |-- areaname: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- FROM: integer (nullable = true)\n",
      " |-- TO: integer (nullable = true)\n",
      " |-- INSEE_COM: integer (nullable = true)\n",
      " |-- NOM_COM: string (nullable = true)\n",
      " |-- IRIS: integer (nullable = true)\n",
      " |-- CODE_IRIS: integer (nullable = true)\n",
      " |-- NOM_IRIS: string (nullable = true)\n",
      " |-- TYP_IRIS: string (nullable = true)\n",
      " |-- geom: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfLinks.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore one sample of the data, to better understand the nature of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfLinks.filter(col(\"INSEE_COM\") == 69389).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfLinks.filter(col(\"INSEE_COM\") == 69383).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfLinks.filter(col(\"NOM_COM\") == \"Villeurbanne\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(dfLinks.filter(col(\"iris_count\") > 5).count())\\ndfLinks.filter(col(\"iris_count\") > 5).show(30)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(dfLinks.filter(col(\"iris_count\") > 5).count())\n",
    "dfLinks.filter(col(\"iris_count\") > 5).show(30)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to prepare our trip dataset for clustering analysis.<br>\n",
    "\n",
    "We would like to identify all observations that belong to a subset of arrondissments around the city of Lyon and neighbor communes. You can decide to focus on other/more/less communes by selecting the areas of interest based on the <code>INSEE_COM</code> or <code>NOM_COM</code> columns of the links data frame.\n",
    "\n",
    "- First, we can decide to filter abnormal speed values (>= 200 kmh).\n",
    "- Then, we will need to retrieve for each start observation the IRIS code of the associated link.\n",
    "- Finally, we will compute a congestion indicator, i.e., the ratio between the measured speed and the link free flow speed.\n",
    "\n",
    "We define all these filters/join operations in the following prepareTripsData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Communes to retain\n",
    "#communesToRetain = [69381, 69382, 69383, 69384, 69385, 69386, 69387, 69388, 69389, 69266]\n",
    "communesToRetain = [69381, 69382, 69383, 69384, 69385, 69386, 69387, 69388, 69389, 69266,\\\n",
    "                    69034, 69202, 69259, 69152, 69199, 69142, 69029, 69149, 69271, 69290,\\\n",
    "                    69256, 69081, 69244, 69089,69040]\n",
    "\n",
    "def prepareTripsData(dfTrips, dfLinks, sampling_time_minutes=15):\n",
    "    dfFilteredTrips = dfTrips.select(\"linkId\", \"year\", \"month\", \"day\", \"hour\", \"minute\", \"speedInKph\")\n",
    "        \n",
    "    with time_usage(\"Filtering abnormal speed values\"):\n",
    "        dfFilteredTrips = dfFilteredTrips.filter((col('speedInKph') < 200) & (col(\"minute\").isNotNull()))\n",
    "        #binning_to_sampling_time_udf = udf(lambda minutes: sampling_time_minutes*(minutes // sampling_time_minutes))\n",
    "        dfFilteredTrips = dfFilteredTrips.withColumn(\"15_minutes_slot\", (sampling_time_minutes * (col(\"minute\") / sampling_time_minutes).cast('integer')))\n",
    "    #dfFilteredTrips.show(100)\n",
    "    \n",
    "    # Some links are splitted over several iris communes\n",
    "    # For simplicity, we will filter the Links file by only retaining the link with \n",
    "    # the largest length within a same zone iris\n",
    "    '''window = Window.partitionBy(\"id\").orderBy(dfLinks[\"iris_length\"].desc())\n",
    "    dfLinks = dfLinks.withColumn(\"rank\", F.row_number().over(window))\n",
    "    dfLinks = dfLinks.filter(dfLinks[\"rank\"] == 1)'''\n",
    "    #dfLinks.filter(col(\"iris_count\") > 5).show(100)\n",
    "    \n",
    "    #let's join now our trips with the insee information associated to the links\n",
    "    dfFilteredTrips = dfFilteredTrips.join(dfLinks, dfFilteredTrips.linkId == dfLinks[\"id\"])\n",
    "    #dfOnlyOriginTripsFiltered.show(5)\n",
    "    dfFilteredTrips.printSchema()\n",
    "    \n",
    "    # let's keep only the trips whose origin falls in one of the selected communes\n",
    "    '''with time_usage(\"Counting the size of the data frame before INSEE filtering\"):\n",
    "        print(\"Original df has size: \" + str(dfOnlyOriginTripsFiltered.count()))'''\n",
    "    with time_usage(\"Filtering by INSEE_COM\"):\n",
    "        dfFilteredTrips = dfFilteredTrips.filter(col('INSEE_COM').isin(communesToRetain))\n",
    "    with time_usage(\"Computing speed ratio\"):\n",
    "        speed_ratio_col = when((col(\"speedInKph\").isNull()) | (col(\"ffs\").isNull()) | (col(\"ffs\") == 0), None) \\\n",
    "                                                 .otherwise(col(\"speedInKph\") / col(\"ffs\"))\n",
    "        dfFilteredTrips = dfFilteredTrips.withColumn(\"speed_ratio\", speed_ratio_col)\n",
    "        \n",
    "    '''with time_usage(\"Counting the size of the data frame after INSEE filtering\"):\n",
    "        print(\"Original df has size: \" + str(dfOnlyOriginTripsFiltered.count()))'''\n",
    "    \n",
    "    dfFilteredTrips.printSchema()\n",
    "    return dfFilteredTrips.select(\"year\", \"month\", \"day\", \"hour\", \"15_minutes_slot\", \"speed_ratio\", \"INSEE_COM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the <code>prepareTripsData</code> to the our selected days of observation. You can decide to further reduce the days, depending on the computing power of your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Filtering anomalous speed values: elapsed seconds: 0.245\n",
      "INFO:root:Filtering by INSEE_COM: elapsed seconds: 0.1572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- linkId: long (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- speedInKph: integer (nullable = true)\n",
      " |-- 15_minutes_slot: integer (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- iris_count: integer (nullable = true)\n",
      " |-- iris_length: double (nullable = true)\n",
      " |-- ffs: integer (nullable = true)\n",
      " |-- speedlimit: integer (nullable = true)\n",
      " |-- frc: integer (nullable = true)\n",
      " |-- netclass: integer (nullable = true)\n",
      " |-- fow: integer (nullable = true)\n",
      " |-- routenumber: string (nullable = true)\n",
      " |-- areaname: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- FROM: integer (nullable = true)\n",
      " |-- TO: integer (nullable = true)\n",
      " |-- INSEE_COM: integer (nullable = true)\n",
      " |-- NOM_COM: string (nullable = true)\n",
      " |-- IRIS: integer (nullable = true)\n",
      " |-- CODE_IRIS: integer (nullable = true)\n",
      " |-- NOM_IRIS: string (nullable = true)\n",
      " |-- TYP_IRIS: string (nullable = true)\n",
      " |-- geom: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing speed ratio: elapsed seconds: 0.0493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- linkId: long (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- speedInKph: integer (nullable = true)\n",
      " |-- 15_minutes_slot: integer (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- iris_count: integer (nullable = true)\n",
      " |-- iris_length: double (nullable = true)\n",
      " |-- ffs: integer (nullable = true)\n",
      " |-- speedlimit: integer (nullable = true)\n",
      " |-- frc: integer (nullable = true)\n",
      " |-- netclass: integer (nullable = true)\n",
      " |-- fow: integer (nullable = true)\n",
      " |-- routenumber: string (nullable = true)\n",
      " |-- areaname: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- FROM: integer (nullable = true)\n",
      " |-- TO: integer (nullable = true)\n",
      " |-- INSEE_COM: integer (nullable = true)\n",
      " |-- NOM_COM: string (nullable = true)\n",
      " |-- IRIS: integer (nullable = true)\n",
      " |-- CODE_IRIS: integer (nullable = true)\n",
      " |-- NOM_IRIS: string (nullable = true)\n",
      " |-- TYP_IRIS: string (nullable = true)\n",
      " |-- geom: string (nullable = true)\n",
      " |-- speed_ratio: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- 15_minutes_slot: integer (nullable = true)\n",
      " |-- speed_ratio: double (nullable = true)\n",
      " |-- INSEE_COM: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filteredDfTrips = dfTrips.filter((col(\"month\") == 10) & (col(\"day\") <= 7))\n",
    "dfTripsPrepared = prepareTripsData(dfTrips, dfLinks)\n",
    "dfTripsPrepared.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have produced a dataframe containing the year, month, day, hour of the day, commune and 15-minutes time slot of the observed speed (ratio) sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering \n",
    "We will cluster now the couples (year, month, day, hour, time-slot) based on the average speed ratio observed over the different communes.\n",
    "The idea is to find moments in time that have similar features of congestion in the selected communes. \n",
    "\n",
    "We hope to find some patterns, such as moments of the day (e.g. morning hours of working days) which are pretty similar in the way traffic is distributed as well as moments of the day (e.g. morning hours of weekend days) that are very peculiar and different from the other ones.\n",
    "\n",
    "Based on the previous prepared dataframe, we will now compute the mean speed ratio in each of the retained areas at a given time slots, by a simple groupby operation (on the fields \"year\", \"month\", \"day\", \"hour\", \"15_minutes_slot\") followed by an average operation per each INSEE_COM. Pivoting is necessary to transform the different means (each related to an INSEE_COM) into columns.\n",
    "\n",
    "We also compute count and standard deviation for analysis purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we first sort the dataframe based on the INSEE_COM and the group by day and hour\n",
    "dfTripsGrouped = dfTripsPrepared.groupby(\"year\", \"month\", \"day\", \"hour\", \"15_minutes_slot\").pivot(\"INSEE_COM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- 15_minutes_slot: integer (nullable = true)\n",
      " |-- 69029_sr_mean: double (nullable = true)\n",
      " |-- 69029_sr_count: long (nullable = true)\n",
      " |-- 69029_sr_std: double (nullable = true)\n",
      " |-- 69034_sr_mean: double (nullable = true)\n",
      " |-- 69034_sr_count: long (nullable = true)\n",
      " |-- 69034_sr_std: double (nullable = true)\n",
      " |-- 69040_sr_mean: double (nullable = true)\n",
      " |-- 69040_sr_count: long (nullable = true)\n",
      " |-- 69040_sr_std: double (nullable = true)\n",
      " |-- 69081_sr_mean: double (nullable = true)\n",
      " |-- 69081_sr_count: long (nullable = true)\n",
      " |-- 69081_sr_std: double (nullable = true)\n",
      " |-- 69089_sr_mean: double (nullable = true)\n",
      " |-- 69089_sr_count: long (nullable = true)\n",
      " |-- 69089_sr_std: double (nullable = true)\n",
      " |-- 69142_sr_mean: double (nullable = true)\n",
      " |-- 69142_sr_count: long (nullable = true)\n",
      " |-- 69142_sr_std: double (nullable = true)\n",
      " |-- 69149_sr_mean: double (nullable = true)\n",
      " |-- 69149_sr_count: long (nullable = true)\n",
      " |-- 69149_sr_std: double (nullable = true)\n",
      " |-- 69152_sr_mean: double (nullable = true)\n",
      " |-- 69152_sr_count: long (nullable = true)\n",
      " |-- 69152_sr_std: double (nullable = true)\n",
      " |-- 69199_sr_mean: double (nullable = true)\n",
      " |-- 69199_sr_count: long (nullable = true)\n",
      " |-- 69199_sr_std: double (nullable = true)\n",
      " |-- 69202_sr_mean: double (nullable = true)\n",
      " |-- 69202_sr_count: long (nullable = true)\n",
      " |-- 69202_sr_std: double (nullable = true)\n",
      " |-- 69244_sr_mean: double (nullable = true)\n",
      " |-- 69244_sr_count: long (nullable = true)\n",
      " |-- 69244_sr_std: double (nullable = true)\n",
      " |-- 69256_sr_mean: double (nullable = true)\n",
      " |-- 69256_sr_count: long (nullable = true)\n",
      " |-- 69256_sr_std: double (nullable = true)\n",
      " |-- 69259_sr_mean: double (nullable = true)\n",
      " |-- 69259_sr_count: long (nullable = true)\n",
      " |-- 69259_sr_std: double (nullable = true)\n",
      " |-- 69266_sr_mean: double (nullable = true)\n",
      " |-- 69266_sr_count: long (nullable = true)\n",
      " |-- 69266_sr_std: double (nullable = true)\n",
      " |-- 69271_sr_mean: double (nullable = true)\n",
      " |-- 69271_sr_count: long (nullable = true)\n",
      " |-- 69271_sr_std: double (nullable = true)\n",
      " |-- 69290_sr_mean: double (nullable = true)\n",
      " |-- 69290_sr_count: long (nullable = true)\n",
      " |-- 69290_sr_std: double (nullable = true)\n",
      " |-- 69381_sr_mean: double (nullable = true)\n",
      " |-- 69381_sr_count: long (nullable = true)\n",
      " |-- 69381_sr_std: double (nullable = true)\n",
      " |-- 69382_sr_mean: double (nullable = true)\n",
      " |-- 69382_sr_count: long (nullable = true)\n",
      " |-- 69382_sr_std: double (nullable = true)\n",
      " |-- 69383_sr_mean: double (nullable = true)\n",
      " |-- 69383_sr_count: long (nullable = true)\n",
      " |-- 69383_sr_std: double (nullable = true)\n",
      " |-- 69384_sr_mean: double (nullable = true)\n",
      " |-- 69384_sr_count: long (nullable = true)\n",
      " |-- 69384_sr_std: double (nullable = true)\n",
      " |-- 69385_sr_mean: double (nullable = true)\n",
      " |-- 69385_sr_count: long (nullable = true)\n",
      " |-- 69385_sr_std: double (nullable = true)\n",
      " |-- 69386_sr_mean: double (nullable = true)\n",
      " |-- 69386_sr_count: long (nullable = true)\n",
      " |-- 69386_sr_std: double (nullable = true)\n",
      " |-- 69387_sr_mean: double (nullable = true)\n",
      " |-- 69387_sr_count: long (nullable = true)\n",
      " |-- 69387_sr_std: double (nullable = true)\n",
      " |-- 69388_sr_mean: double (nullable = true)\n",
      " |-- 69388_sr_count: long (nullable = true)\n",
      " |-- 69388_sr_std: double (nullable = true)\n",
      " |-- 69389_sr_mean: double (nullable = true)\n",
      " |-- 69389_sr_count: long (nullable = true)\n",
      " |-- 69389_sr_std: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as func\n",
    "dfTripsGrouped = dfTripsGrouped.agg(func.mean('speed_ratio').alias('sr_mean'), func.count('speed_ratio').alias('sr_count'), \\\n",
    "                                       func.stddev('speed_ratio').alias('sr_std'))\n",
    "dfTripsGrouped.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we won't perform feature scaling, as speed ratios are already relative measures (ratio of the speed and the free flow over the links). \n",
    "\n",
    "Measures larger than one are however possibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling features\n",
    "\n",
    "In order to run Spark’s k-means clustering algorithm, a single input column with “features” column of Vector type must be given. Now, the features are several separate columns (the average speed ratio per commune per time-slot). \n",
    "\n",
    "To merge them into a single Vector column, Spark’s VectorAssembler transformation is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[str(x) + \"_sr_mean\" for x in communesToRetain], outputCol=\"features\", handleInvalid = \"skip\")\n",
    "dfTripsFeatures = assembler.transform(dfTripsGrouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the assembly operation has been performed, we are going to apply the KMeans algorithm on the assembled features, using NUM_CLUSTERS clusters and a seed = 1 for reproducibility reasons.\n",
    "\n",
    "The ml API to apply machine learning algorithms in Pyspark is very similar to the sckit learn one:\n",
    "- first you fit the model to train your algorithm (in this case to apply the K-Means algorithm to the selected features of your trainning set.\n",
    "- then you apply transform on the trained model to retrieve the cluster labels in the prediction field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfTripsGrouped.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfTripsFeatures.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans, BisectingKMeans\n",
    "\n",
    "NUM_CLUSTERS = 16\n",
    "INITIAL_SEED = 1\n",
    "\n",
    "# Fitting the model to apply K-means\n",
    "with time_usage(\"Performing K-means clustering\"):\n",
    "    #kmeans = KMeans(k=NUM_CLUSTERS, seed=INITIAL_SEED)\n",
    "    kmeans = BisectingKMeans(k=NUM_CLUSTERS, seed=INITIAL_SEED).setFeaturesCol(\"features\")\n",
    "    model = kmeans.fit(dfTripsFeatures.select('features'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>clusterCenters</code> function from the KMeans trained model retrieves the centers of the NUM_CLUSTERS cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.clusterCenters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve the clestering labels associated to the time slots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the model to retrieve the labels\n",
    "with time_usage(\"Retrieving K-means labels\"):\n",
    "    dfClustered = model.transform(dfTripsFeatures)\n",
    "#dfClustered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the results to a parquet dataframe, partitioned by cluster. <br>\n",
    "In the following code, choose the proper filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILENAME = \"/Users/furno/Desktop/clusteredTrips\" \n",
    "'''dfClustered.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .partitionBy(\"prediction\") \\\n",
    "    .save('file:///' + OUTPUT_FILENAME)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the final dataframe has a much reduced size than the original one thanks to our processing, we can think of converting it to a pandas dataframe and save it in a simple csv file, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfClusteredTripsPandas = dfClustered.toPandas()\n",
    "#dfClusteredTripsPandas.to_csv(OUTPUT_FILENAME + '/../clustered_dataframe.csv', index=False)\n",
    "dfClusteredTripsPandas.to_csv('clustered_dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfClusteredTripsPandas = pd.read_csv('clustered_dataframe.csv')\n",
    "dfClusteredTripsPandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing and plotting the clusters \n",
    "\n",
    "In the previos steps, we have clustered points that correspond to moments in time, i.e., (hour, day) pairs, based on the distributions of the emitted demand from our selected communes.\n",
    "\n",
    "The best way to analyze the cluster results appears to be the following:\n",
    "1. plotting with a different color each time slot,\n",
    "2. plotting with an histogram the associated traffic distribution over the selected communes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfClusteredTripsPandas[\"day\"] = dfClusteredTripsPandas[\"day\"].apply(lambda x: '{0:0>2}'.format(x))\n",
    "dfClusteredTripsPandas[\"month\"] = dfClusteredTripsPandas[\"month\"].apply(lambda x: '{0:0>2}'.format(x))\n",
    "dfClusteredTripsPandas[\"hour\"] = dfClusteredTripsPandas[\"hour\"].apply(lambda x: '{0:0>2}'.format(x))\n",
    "dfClusteredTripsPandas[\"15_minutes_slot\"] = dfClusteredTripsPandas[\"15_minutes_slot\"].apply(lambda x: '{0:0>2}'.format(x))\n",
    "dfClusteredTripsPandas[\"the_date\"] = dfClusteredTripsPandas[[\"year\", \"month\", \"day\"]].apply(lambda row: '-'.join(row.values.astype(str)), axis=1)\n",
    "dfClusteredTripsPandas[\"the_hour\"] = dfClusteredTripsPandas[[\"hour\", \"15_minutes_slot\"]].apply(lambda row: ':'.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "dfClusteredTripsPandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. plotting the clusters over time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "font = {'size' : 50}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "temp_df = dfClusteredTripsPandas.pivot(index='the_date', columns='the_hour', values='prediction')\n",
    "temp_df.sort_index(inplace=True) \n",
    "print(temp_df.head() )\n",
    "\n",
    "fig = plt.figure(figsize = (300,100))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "cax = ax.matshow(temp_df.values, cmap=\"Spectral\")\n",
    "ax.xaxis.set_ticks_position('bottom') \n",
    "\n",
    "col_labels = temp_df.columns.values\n",
    "x_labels = [(col_labels[i] if (i%2 == 1) else \"\") for i in range(len(col_labels))]\n",
    "ax.set_xticks(range(len(col_labels)))\n",
    "ax.set_xticklabels(x_labels, rotation=90) \n",
    "\n",
    "row_labels = temp_df.index.values\n",
    "y_labels = [(row_labels[i]  if (i % 2 == 0 or i == len(row_labels) - 1) else \"\") for i in range(len(row_labels))] \n",
    "ax.set_yticks(range(len(row_labels)))\n",
    "ax.set_yticklabels(y_labels)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. plotting clusters'centroid\n",
    "import pandas as pd\n",
    "\n",
    "dfLinks = pd.read_csv(LINKS_FILEPATH)\n",
    "communes = pd.Series(dfLinks[\"NOM_COM\"].values,index=dfLinks[\"INSEE_COM\"]).to_dict()\n",
    "print(communes)\n",
    "\n",
    "selected_columns = [c for c in dfClusteredTripsPandas.columns if \"sr_mean\" in c]\n",
    "selected_columns.append(\"prediction\")\n",
    "print(selected_columns)\n",
    "\n",
    "dfClusteredTripsPandas = dfClusteredTripsPandas[selected_columns]\n",
    "dfClusteredTripsPandasJoined = dfClusteredTripsPandas.rename(columns={str(key) + \"_sr_mean\":value for (key,value) in communes.items()})\n",
    "\n",
    "dfClusteredTripsPandasJoined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'size': 10}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "dfClusteredTripsPandasJoined = dfClusteredTripsPandasJoined.groupby('prediction').mean()\n",
    "dfClusteredTripsPandasJoined.head(20)\n",
    "axes = dfClusteredTripsPandasJoined.plot.barh(rot=0, figsize = (24,24))\n",
    "plt.title(\"Average speed per commune per each cluster\", bbox={'facecolor':'0.8', 'pad':5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dfClusteredList = [dfClusteredTripsPandasJoined]\n",
    "numClusters = [16]\n",
    "for n in range(6, 35, 4):\n",
    "    # Fitting the model to apply K-means\n",
    "    with time_usage(\"Performing K-means clustering with \" + str(n) + \" clusters:\"):\n",
    "        kmeans_n = KMeans(k=n, seed=INITIAL_SEED)\n",
    "        model_n = kmeans_n.fit(dfTripsFeatures.select('features'))\n",
    "        dfClustered_n = model.transform(dfTripsFeatures)\n",
    "        dfClusteredTripsPandas_n = dfClustered_n.toPandas()\n",
    "        dfClusteredTripsPandas_n.to_csv(OUTPUT_FILENAME + '/../n_clusters/clustered_dataframe_' + str(n) + '.csv', index=False)\n",
    "        dfClusteredList.append(dfClusteredTripsPandas_n)\n",
    "        numClusters.append(n)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
